<!--
link: https://cdn.jsdelivr.net/gh/posetgar/preserving_data_training_test@main/style/ugent-theme.css

title: Research Data Management in practice
author:     Paula Oset Garcia
email:      rdm.support@ugent.be
comment:    This course introduces...
icon:       https://styleguide.ugent.be/files/uploads/logo_UGent_EN_RGB_2400_kleur_witbg.png
repository: https://github.com/posetgar/preserving_data_course
language:   en
@MERMAID:   true
mode: section
-->

#  RDM Training Design & Evaluation in Practice

This course corresponds to Module 4.2 in the [University of Vienna Data Steward Certificate Course.](https://rdm.univie.ac.at/data-stewards-at-the-university/become-a-data-steward/)

---

<h2>Course information</h2>

<h3>Course description</h3>
This session provides a practical introduction to designing and evaluating training in the domain of Research Data Management (RDM). It builds on the basic didactic concepts introduced in the previous session (4.1) and applies them through the design of an RDM training. In this follow-up session, the focus shifts from theory to practice. We will concentrate on actively thinking through how RDM training can be designed, adapted, and evaluated in your own context. Working in small groups, you will identify training needs, define the characteristics of your target audience, formulate learning objectives, and explore suitable learning activities.

By the end of the session, you will have taken the first steps toward developing an RDM training concept that is aligned with learner needs, supported by clear outcomes, and grounded in thoughtful instructional design.

During the session, the following topics will be addressed:

- Training example: mapping an RDM course delivered by Ghent University data stewards to the ABC learning design method  
- Overview of different RDM learning activities  
- Evaluating and adapting training: how to include evaluation in training and adapt according to feedback or delivery method  
- Hands-on work: getting started with the design of RDM training


<div class="ugent-box ugent-example">
<h3>Learning outcomes</h3>
<p>Upon completion of this session, students will be able to:</p>

<p>- Explain why investing time in training design is essential and describe the overall training lifecycle (from needs analysis to evaluation)</p>
<p>- Analyze the characteristics and needs of the target audience</p>
<p>- Identify and define the topics, learning objectives, and outcomes for the course</p>
<p>- Propose appropriate learning activities and learning types for the different course topics</p>
<p>- Sketch a plan to include assessment and feedback mechanisms for the course</p> 
<p>- Evaluate the effectiveness of the learning activities proposed by peers</p>
<p>- Process and incorporate feedback into their own training design or concept</p> 
<p>- Develop a sense of ownership and responsibility for their own learning</p>
</div>


<h3>Methods</h3>
This lesson includes short lectures and group discussion, as well as smaller group work tasks. It is an interactive session involving knowledge-sharing and discussion among learners.

<h3>Assignment</h3>
Individually designing a training concept using the template given (2-3 pages).
The details of the assignment will be discussed during the live session and uploaded to Moodle.

<div class="ugent-box ugent-example">
<h3>Example</h3>
<p>This is an example with UGent styling.</p>
</div>

<div class="ugent-box ugent-activity">
<h3>Activity</h3>
<p>Try applying the ADDIE model to your own training.</p>
</div>

<div class="ugent-box ugent-learning">
<h3>Learning Outcome</h3>
<p>You will be able to identify key instructional design principles.</p>
</div>

<div class="ugent-box ugent-warning">
<h3>Warning</h3>
<p>Do not skip the analysis phase—it saves time later!</p>
</div>

## Why spending time on training design?

In the last two previous years, we asked participants of the module how much time they spent on different aspects of training design? These were the results:
Scale

- **1 ->** I do not spend any time on this aspect
- **2 ->** I spend **little** time on this aspect
- **3 ->** I spend a **moderate** amount of time on this aspect
- **4 ->** I spend **quite a lot** of time on this aspect
- **5 ->** I spend **most** of my time on this aspect

<div style="width:100%; text-align:center; margin: 1em 0;">
  <iframe 
    src="https://posetgar.github.io/RDM_training_design/charts/radar.html"
    style="width:100%; height:700px; border:none; overflow:hidden;"
    scrolling="no"
    allow="fullscreen"
  ></iframe>
</div>


| Aspect                               | Average (2024–2025) |
| ------------------------------------ | -------------------:|
| Analyzing training needs             |                 2.9 |
| Defining learning objectives/outcomes|                 2.7 |
| Defining the audience                |                 2.6 |
| **Creating content & learning activities** |               **4.3** |
| Designing assessment methods         |                 2.1 |
| Training evaluation                  |                 1.8 |

As you can see, most people spend most of their time in creating content, spend the least ammount of time in assessment and evaluation, and spend limited ammount of time in the training design phase.


<h3> Why spending time on training design? </h3>

Good training does not happen by accident. Investing time in training design ensures that the learning experience is purposeful, relevant, and effective. By consciously planning who the training is for, what it should achieve, and how it will be delivered, you create a structure that supports both the trainer and the learners.

<details>
  <summary><strong>1. Align training with real needs</strong></summary>

If you understand who your learners are and what they struggle with, you can tailor the training to what truly matters.

**Tip:** Instead of giving a generic RDM overview to postdocs, you focus on *GDPR in practice* and *repository selection* because that’s where they typically struggle.

</details>

<details>
  <summary><strong>2. Define clear learning outcomes</strong></summary>

Clear outcomes help you decide what to include, what to skip, and how to check whether learning occurred.

**Tip:** Instead of “understand DMPs”, define: *“Learners can complete the data summary table for their own project.”* For policy sessions: *“Learners can identify which RDM and Open Science policies apply to their project.”*

</details>

<details>
  <summary><strong>3. Choose appropriate learning activities</strong></summary>

Spending time on designing makes you plan **how** learners will learn, not just **what** you will present. Learning activities become meaningful, because they support specific outcomes.

**Tip:** Rather than only presenting slides about file naming, use a short activity where they fix poorly named files. Instead of just showing a FAIR checklist, ask learners to compare datasets in repositories and discuss FAIRness.

</details>

<details>
  <summary><strong>4. Increase engagement and motivation</strong></summary>

Good design keeps learners active rather than passive.

**Tip:** Break long webinars into short blocks of content, and include polls, quizzes, or breakout discussions. Let participants bring **their own RDM issues** and discuss them in small groups.

</details>

<details>
  <summary><strong>5. Make evaluation and improvement possible</strong></summary>

With clear goals and structure, it is easier to assess whether the training worked — and to adapt based on feedback or delivery mode.

**Tip:** Instead of collecting only an end‑of‑course survey, include assessment/feedback moments during the session than can help you evaluate specific activities. After each edition, document what to keep, drop, or change.

</details>

<details>
  <summary><strong>6. Use time and resources efficiently</strong></summary>

Thoughtful design prevents overload, avoids unnecessary content, and focuses on what truly matters.

**Tip:** Rather than preparing an extensive slide deck covering all aspects of RDM, you may choose to focus on one key problem to work on (e.g. storage). You can also design reusable activities that can be adapted for future sessions. 

</details>

<br>

## The training design lifecycle

Training design is more than preparing slides or selecting activities. It is a
structured process that helps you create learning experiences that are relevant,
effective, and aligned with the needs of your audience. Most training design
approaches share a set of common components and follow a recurring cycle.

<div class="ugent-box ugent-warning">
<h3>Key components of a training design cycle</h3>
<p>- **Analysis**: Understanding your learners, their context, and their needs.</p>
<p>- **Design**: Translating needs into clear learning outcomes and selecting appropriate methods.</p>
<p>- **Development**: Creating learning materials, activities, and assessments.</p>
<p>- **Implementation**: Delivering the training in practice.</p>
<p>- **Evaluation**: Gathering feedback and assessing the effectiveness of the training.</p>
</div>

These steps appear in many instructional design models. One of the most well‑known examples is the **ADDIE framework** (Analysis – Design – Development – Implementation – Evaluation). Other models (e.g., SAM, Backward Design, 5Es) offer similar guidance but with different emphases.

<h3> Why use a model? </h3>

A training design model helps you:

- **Stay focused** on the purpose of the training.
- **Ensure alignment** between learning needs, outcomes, methods, and assessments.
- **Avoid jumping prematurely into content creation**, which often leads to overload or materials that do not support learning.
- **Build a structure that supports evaluation and continuous improvement**.

Models give you a roadmap, but they are not meant to be followed rigidly. It's important to keep in mind that training design is **iterative**, not linear. Many models start with *Analysis* and end with *Evaluation*. In practice, **evaluation is actually at the core** and you will often:

- revisit earlier decisions,
- refine learning outcomes after testing activities,
- adjust methods based on learner feedback,
- update materials when changing context or delivery format.

<br>
<img src="https://raw.githubusercontent.com/posetgar/preserving_data_training_test/main/images/Training_lifecycle.png"
     alt="Training lifecycle" style="max-width: 80%; display: block; margin: 1rem auto;" />
<br>


It is tempting to start with *Development* (e.g., making slides), but investing time in *Analysis* and *Design* saves effort later and results in a stronger learning experience.

<h3> ABC Learning design </h3>

In the previous module, you were introduced to the [ABC Learning Design method](https://abc-ld.org/). The ABC Learning Design is a lightweight instructional design framework developed at UCL. It helps you rapidly map a training session using six learning activity types (Acquisition, Collaboration, Discussion, Investigation, Practice, Production) The assignment of this module will be based on this method, so here's a quick recap to explain why we'll use it.

<h4> Why use the ABC for RDM training design? </h4>

<details>
  <summary><strong>It gives quick, flexible visual results</strong></summary>

ABC is designed for short, fast-paced planning workshops. You can outline an entire session in under an hour. The card-based, storyboard-like process produces a clear session outline immediately. This makes it easy to share with colleagues, adjust, and reuse.

</details>

<details>
  <summary><strong>You don’t need a pedagogical background</strong></summary>

The model uses simple, intuitive activity types. This makes it accessible for RDM professionals who design training but might not be formal educators.

</details>

<details>
  <summary><strong>It supports learner‑centred design</strong></summary>

ABC focuses on what learners do, not just on what trainers deliver. This helps make RDM training more engaging and relevant, even when time with learners is limited.

</details>

<details>
   <summary><strong>It encourages diversity in learning activities</strong></summary>

The model uses simple, intuitive activity types. This makes it accessible for RDM professionals who design training but aren’t formal educators.

</details>

<br>

<h3>What comes next</h3>
        
In the next part of the course, we will explore the first two stages of the training design cycle (*Analysis* and *Design*), which are covered by the ABC model, and apply these steps to develop your own training concept.
        

## Initial training analysis

Before starting to design activities or content, it’s important to pause and carry out a short initial training analysis. This helps you clarify the purpose of your training, understand who your learners are, and identify any practical constraints. Even a simple analysis makes the later design phase (including ABC Learning Design) faster, more focused, and much more effective.

<div class="ugent-box ugent-warning">
<h3>What to condsider in an initial training analysis?</h3>
<p>- **Goals** of your training – the big picture: why this training exists and what change you hope it will bring.</p>
<p>- **Topics** and scope – what the training will broadly address.</p>
<p>- **Audience** – who the learners are, what they already know, what they need, and what might motivate or block them.</p>
<p>- **Learning outcomes** – what learners should be able to do afterwards.</p>
<p>- **Delivery choices and practical issues** – time, tools, format, available resources, and constraints.</p>
</div>

This information feeds directly into ABC storyboarding later on: clearer goals and outcomes make it easier to select appropriate activity types and to avoid irrelevant content.

<div class="ugent-box ugent-example">
<h3>Goals vs. outcomes</h3>
<p>When planning training, you’ll often see the terms goals, objectives, and learning outcomes used. Sometimes the terms overlap or are even interchanged — especially objectives and outcomes. To keep things practical and usable, in this module we focus mainly on goals and outcomes: they may sound similar, but they serve different purposes.</p>
<h4>Goals: your big-picture intention</h4>
<p>A goal expresses the broad purpose of your training. It os often written from an instructor's or institution's perspective and answers the question:
“Why are we offering this training in the first place?”. These are large ambitions — your training contributes to them, but does not achieve them alone. Examples: "Strengthen good research data management practices accross the institution", "Support more transparent and reproducible research".</p>
<h4>Outcomes: what learners will be able to do afterwards</h4>
<p>Learning outcomes describe the specific, observable things learners should be able to do after completing the training. They are observable behaviours or skills that show the training has worked. Examples: "“Participants can choose an appropriate repository for their data", “Participants can recognise when personal or sensitive data requires extra protection”.</p>
</div>


### Writing good learning outcomes
Good learning outcomes help you design focused, measurable training. They guide your choice of activities in the ABC model and help you evaluate whether the training worked.

<h4>Use the SMARTIE framework</h4>
Learning outcomes should be:
- **S**pecific – focused on a clear behaviour or skill
- **M**easurable – observable in some way
- **A**chievable – realistic for your timeframe
- **R**elevant – tied to your training goal
- **T**ime-limited – achievable by the end of this session
- **I**nclusive – accessible to all intended learners
- **E**quitable – fair regardless of background or prior experience

<h4>Use Bloom's Taxonomy to choose strong action verbs</h4>
Avoid vague verbs like “know”, “learn about”, “understand”, which are hard to measure. Bloom’s levels go from basic to advanced cognitive activities:

<div class="ugent-box ugent-example">
<h3>Remember</h3>
<p>**Useful action verbs**: cite, define, describe, identify, list, recall.</p>
<p>**Example**: "*Learners will understand what a DMP is*" vs. "*Learners can **identify** the mandatory sections of a data management plan*".</p>
<p>**SMARTIE**: **S**pecific (mandatory sections); **M**easurable verb; **A**chievable in a short lesson; **R**elevant to all researchers.</p>
</div>

When planning training, you’ll often see the terms goals, objectives, and learning outcomes used. Sometimes the terms overlap or are even interchanged — especially objectives and outcomes. To keep things practical and usable, in this module we focus mainly on goals and outcomes: they may sound similar, but they serve different purposes.</p>

- Remember: list, identify, define
- Understand: explain, describe, summarise
- Apply: use, implement, demonstrate
- Analyze: compare, distinguish, examine
- Evaluate: assess, justify, critique
- Create: design, develop, construct

| Bloom level | Useful action verbs | Weak outcome | Improved outcome | Why better (SMARTIE) |
|-------------|----------------------------------|--------------|------------------|-----------------------|
| **Remember** | cite, define, describe, identify, list, recall | “Learners will understand what a DMP is.” | “Learners can **identify** the mandatory sections of a data management plan.” | **S**pecific (mandatory sections); **M**easurable verb; **A**chievable in a short lesson; **R**elevant to all researchers. |
| **Understand** | classify, compare, describe, discuss, explain, summarize | “Learners will know why file naming matters.” | “Learners can **explain** two reasons why consistent file naming improves research workflow.” | **S**pecific (two reasons); **M**easurable explanation; **I**nclusive—no prior experience needed. |
| **Apply** | apply, choose, demonstrate, implement, organize, use | “Participants will learn about backup options.” | “Participants can **apply** the 3‑2‑1 rule to propose a backup strategy for a simple research scenario.” | **R**elevant (resembles real cases); **M**easurable; **A**chievable using a scenario. |
| **Analyze** | analyze, categorize, compare, differentiate, distinguish, examine | Learners will understand why documentation is important.” | Learners can **compare** two datasets with different levels of documentation quality and **explain** how this affects their reusability.” | **S**pecific comparison task; **M**easurable differences in reusability. |
| **Evaluate** | assess, critique, decide, evaluate, justify, recommend | “Learners will know which repository to use.” | “Learners can **evaluate** three repository options and **justify** which one best fits a given dataset.” | **M**easurable via justification; **R**elevant to real tasks; **T**ime‑limited (three options). |
| **Create** | create, design, develop, produce, revise, write | “Learners will learn how to make data FAIR.” | “Learners can **design** a short FAIR‑improvement plan for a dataset, including at least two concrete actions for ‘Findable’ or ‘Reusable’.” | **S**pecific output; **M**easurable (two actions); **R**elevant to real workflows. |



<h4>Practice: re-write poorly written learning outcomes</h4>
Below you see a poorly written learning outcome, followed by a better version with missing words. Choose the correct terms from the dropdown(s).

<h5>Data management plans</h5>
- Learners will understand DMPs.
- By the end of this session, you will be able to [[(identify)|know|memorize]] the key components of a Data Management Plan.

<h5>Senstitive data awareness</h5>
- Learners will appreciate data sensitivity.
- By the end of this session, you will be able to [[(explain)|note|list]] when data is considered sensitive and [[(justify)|imagine|assume]] the additional protections it requires.

<h5>Data repositories</h5>
- Participants will learn about data repositories.
- By the end of this training, you will be able to [[(compare)|remember|observe]] repository options and [[(select)|speculate|guess]] one that fits a given scenario.

<h5>FAIR assessment</h5>
- Participants will understand FAIR.
- By the end of the workshop, you will be able to [[(evaluate)|mention|copy]] a dataset against the FAIR principles and [[(recommend)|outline|speculate]] one improvement.

<h5>Licensing for data sharing</h5>
- Learners will know about licenses.
- By the end of this session, you will be able to [[(choose)|observe|remember]] an appropriate license for sharing a dataset and [[(justify)|describe|assume]] your choice.

<br>
<h3>References</h3>
- DePaul University. Course objectives & learning outcomes. https://resources.depaul.edu/teaching-commons/teaching-guides/course-design/Pages/course-objectives-learning-outcomes.aspx 
- Bruna Piereck, Jill Jaworski, Nina Norgren, Elin Kronander, Kristen Schroeder, Alexander Botzki, Jessica Lindvall, Ineke Luijten (2025). The ELIXIR Training material made FAIR by design course. DOI: 10.5281/zenodo.13773159
- Clare, H., Cepinskas, L., Leenarts, E., & Krzyzanek, M. (2022, October 26). EOSC Synergy Online Training Handbook materials. Zenodo. https://doi.org/10.5281/zenodo.7254522 

### Group activity: Initial training analysis

Now that you have seen what goes into an initial training analysis, it’s time to put this into practice. In this activity, you will work in small groups in breakout rooms to complete a training analysis. The initial training analysis template that will be used for this activity is a simpler version than the one you will use for your individual assignment later in the module, but it includes all the key elements you need to begin thinking like a training designer.

You will work collaboratively on a shared **Miro board**. Each group will receive:
<ul>
  <li>a predefined training scenario,</li>
  <li>a simplified analysis table (Title, Audience, Training goal(s), Benefits/outcomes, Pre‑requisites, Content),</li>
  <li>a set of post‑its to brainstorm and add your ideas to each category.</li>
</ul>

This exercise helps you translate a broad training need into a clearer, structured concept — the essential first step before applying the ABC Learning Design method.

<br>
<img src="https://raw.githubusercontent.com/posetgar/RDM_training_design/main/assets/Initial_training_analysis.png"
     alt="Training lifecycle" style="max-width: 80%; display: block; margin: 1rem auto;" />
<br>

<h4> Tips and guidance for the activity </h4>

- **Stay close to the scenario:**  
  Don’t spend time reinventing or expanding the scenario. Use it as given so you can focus on the analysis rather than defining a new training topic.

- **Avoid getting stuck on institutional differences:**  
  Everyone comes from different contexts. For this activity, agree as a group on shared assumptions (e.g., target audience, tools available). The goal is to practise the process, not to represent your institution perfectly.

- **You decide the variables:**  
  Within the scenario, your group defines the audience, any pre‑requisites, the training goal, the expected outcomes, and the rough content. The scenario sets the direction, but the design choices are yours.

- **Focus only on the training component:**  
  In practice, achieving an institutional goal (e.g. increasing data deposition or supporting a new policy) usually involves multiple actions — communication campaigns, stakeholder engagement, guidance materials, technical support, etc. For this activity, concentrate only on the training session. Do not try to design the full strategy around it.

- **Keep the session realistic:**  
  The session should be **no longer than 90 minutes** and include **no more than 4–5 learning outcomes**. This ensures a manageable scope and enough time for active learning.

- **Think visually and collaboratively:**  
  Use the post‑its to capture ideas quickly. You can refine them later. Aim for clarity, not perfection.